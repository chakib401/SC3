{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!git clone https://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@github.com/chakib401/gcc.git\n",
        "!pip install ogb\n",
        "%cd /content/gcc/\n",
        "!gdown --id \"16mxQrpwp85ovdQuE9ZSzeOXCoAr1kM05\"\n",
        "!unzip -o data.zip -d data"
      ],
      "metadata": {
        "id": "x4GSlYrO0-CI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZEMqwh3H14p1"
      },
      "outputs": [],
      "source": [
        "from ogb.nodeproppred import NodePropPredDataset\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "import scipy.io as io\n",
        "import os\n",
        "\n",
        "def datagen(dataset):\n",
        "  if dataset in ['wiki', 'pubmed', 'computers', 'acm', 'dblp']: \n",
        "    data = io.loadmat(os.path.join('data', f'{dataset}.mat'))\n",
        "    features = data['fea'].astype(float)\n",
        "    adj = data.get('W')\n",
        "    if adj is not None:\n",
        "      adj = adj.astype(float)\n",
        "      if not sp.issparse(adj):\n",
        "          adj = sp.csc_matrix(adj)\n",
        "    \n",
        "    if not sparse and sp.issparse(features):\n",
        "        features = features.toarray()\n",
        "    labels = data['gnd'].reshape(-1) - 1\n",
        "    n_classes = len(np.unique(labels))\n",
        "    return adj, features, labels, n_classes\n",
        "\n",
        "  if dataset == 'arxiv': \n",
        "    from ogb.nodeproppred import NodePropPredDataset\n",
        "    dataset = NodePropPredDataset(name='ogbn-arxiv', root='data')\n",
        "    graph = dataset[0] \n",
        "    data = graph[0]\n",
        "    labels = graph[1].reshape(-1)\n",
        "\n",
        "    \n",
        "    features = data['node_feat']\n",
        "    row_ind = data['edge_index'][0]\n",
        "    col_ind = data['edge_index'][1]\n",
        "    data = np.ones(len(row_ind))\n",
        "    \n",
        "    N = M = len(features)\n",
        "    adj = csr_matrix((data, (row_ind, col_ind)), shape=(M, N))\n",
        "    adj = (adj + adj.T)\n",
        "    \n",
        "    n_classes = len(np.unique(labels))\n",
        "\n",
        "    return adj, features, labels, n_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qtJqU0XL1LWx"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.extmath import randomized_svd\n",
        "from sklearn.preprocessing import normalize\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics import normalized_mutual_info_score as nmi\n",
        "from sklearn.metrics import adjusted_rand_score as ari, davies_bouldin_score\n",
        "from numpy.linalg import inv as inverse\n",
        "from numpy.linalg import norm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.kernel_approximation import PolynomialCountSketch, Nystroem\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from time import time\n",
        "from scipy import sparse\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "def preprocess_dataset(adj, features, row_norm=True, sym_norm=True, feat_norm='l2', tf_idf=False, sparse=False, alpha=1, beta=1):\n",
        "    if sym_norm:\n",
        "        adj = aug_normalized_adjacency(adj, True, alpha=alpha)\n",
        "    if row_norm:\n",
        "        adj = row_normalize(adj, True, alpha=beta)\n",
        "\n",
        "    if tf_idf:\n",
        "        features = TfidfTransformer(norm=feat_norm).fit_transform(features)\n",
        "    else:\n",
        "        features = normalize(features, feat_norm)\n",
        "    \n",
        "    if not sparse:\n",
        "        features = features.toarray()\n",
        "    return adj, features\n",
        "\n",
        "def aug_normalized_adjacency(adj, add_loops=True, alpha=1):\n",
        "    if add_loops:\n",
        "        adj = adj + alpha*sp.eye(adj.shape[0])\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    row_sum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(row_sum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    return d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt).tocoo()\n",
        "\n",
        "\n",
        "def row_normalize(mx, add_loops=True, alpha=1):\n",
        "    if add_loops:\n",
        "        mx = mx + alpha * sp.eye(mx.shape[0])\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx\n",
        "\n",
        "\n",
        "def clustering_accuracy(y_true, y_pred):\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "    def ordered_confusion_matrix(y_true, y_pred):\n",
        "      conf_mat = confusion_matrix(y_true, y_pred)\n",
        "      w = np.max(conf_mat) - conf_mat\n",
        "      row_ind, col_ind = linear_sum_assignment(w)\n",
        "      conf_mat = conf_mat[row_ind, :]\n",
        "      conf_mat = conf_mat[:, col_ind]\n",
        "      return conf_mat\n",
        "\n",
        "    conf_mat = ordered_confusion_matrix(y_true, y_pred)\n",
        "    return np.trace(conf_mat) / np.sum(conf_mat)\n",
        "\n",
        "\n",
        "def square_feat_map(z, c=1):\n",
        "  polf = PolynomialFeatures(include_bias=True)\n",
        "  x = polf.fit_transform(z)\n",
        "  coefs = np.ones(len(polf.powers_))\n",
        "  coefs[0] = c\n",
        "  coefs[(polf.powers_ == 1).sum(1) == 2] = np.sqrt(2)\n",
        "  coefs[(polf.powers_ == 1).sum(1) == 1] = np.sqrt(2*c) \n",
        "  return x * coefs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1HXnpwN988C",
        "outputId": "96f8bd00-2e5b-4e30-85ea-615f7eb7a013"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3025 & 16153 & 1870 & 3 & 1.1\\\n",
            "4057 & 2502276 & 334 & 4 & 1.6\\\n",
            "13381 & 259159 & 767 & 10 & 17.5\\\n",
            "19717 & 64041 & 500 & 3 & 1.9\\\n",
            "2405 & 14001 & 4973 & 17 & 45.1\\\n",
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 0.08 GB: 100%|██████████| 81/81 [00:08<00:00,  9.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/arxiv.zip\n",
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 9532.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "169343 & 1327142 & 128 & 40 & 942.1\\\n"
          ]
        }
      ],
      "source": [
        "for dataset in ['acm', 'dblp', 'computers', 'pubmed',  'wiki', 'arxiv']:\n",
        "  adj, features, labels, n_classes =  datagen(dataset)\n",
        "  d = sp.diags(adj.diagonal())\n",
        "  adj = (adj-d)\n",
        "  a = np.unique(labels, return_counts=True)[1]\n",
        "  print(f'{features.shape[0]} & {adj.getnnz()//2+d.getnnz()} & {features.shape[1]} & {n_classes} & {a.max()/a.min():.1f}\\\\')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eJ39gyf-TXvg"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "def ppmi(x):\n",
        "  c = x.T @ x\n",
        "  col_sums = c.sum(0)\n",
        "  row_sums = c.sum(1)\n",
        "  global_sum = c.sum()\n",
        "  c = c * global_sum / col_sums / row_sums[:, None]\n",
        "  c = np.log(c)\n",
        "  c = np.nan_to_num(c)\n",
        "  return np.maximum(c, 0)\n",
        "\n",
        "\n",
        "def run_model(H, c, k, coclustering=False):\n",
        "  H = StandardScaler(with_std=False).fit_transform(H)\n",
        "  U,_,Vt = randomized_svd(H, k)\n",
        "  \n",
        "  \n",
        "  Z = square_feat_map(U, c=c)\n",
        "  r = Z.sum(0)\n",
        "  D = Z @ r \n",
        "  Z_hat = Z / D[:,None]**.5\n",
        "  svd = TruncatedSVD(k+1)\n",
        "  svd.fit(Z_hat.T)\n",
        "  row_rep = svd.components_.T[:,1:]\n",
        "  row_part = KMeans(k).fit_predict(row_rep)  \n",
        "\n",
        "  \n",
        "  \n",
        "  if coclustering:\n",
        "    V = Vt.T\n",
        "    del Z\n",
        "    Y = square_feat_map(V, c=c)\n",
        "    r = Y.sum(0)\n",
        "    D = Y @ r \n",
        "    Y_hat = Y / D[:,None]**.5\n",
        "    svd = TruncatedSVD(k+1)\n",
        "    svd.fit(Y_hat.T)\n",
        "    col_rep = svd.components_.T[:,1:]\n",
        "    col_part = KMeans(k).fit_predict(col_rep)\n",
        "\n",
        "    return row_part, row_rep, col_part, col_rep\n",
        "\n",
        "  \n",
        "  return row_part, row_rep"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.kernel_approximation import RBFSampler, PolynomialCountSketch, Nystroem\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "from time import time\n",
        "from scipy import sparse\n",
        "from sklearn.metrics import silhouette_score\n",
        "import tensorflow as tf\n",
        "from scipy.spatial.distance import pdist\n",
        "\n",
        "@tf.function\n",
        "def convolve(feature, adj_normalized, power):\n",
        "  for _ in range(power):\n",
        "    feature = tf.sparse.sparse_dense_matmul(adj_normalized, feature)\n",
        "  return feature\n",
        "\n",
        "n_runs = 1\n",
        "\n",
        "\n",
        "for dataset, max_power in [ \n",
        "      ('acm', 100),\n",
        "      ('dblp', 100),\n",
        "      ('arxiv', 100),\n",
        "      ('computers', 100),\n",
        "      ('wiki', 100),\n",
        "      ('pubmed', 100),\n",
        "    ]:\n",
        "  \n",
        "    print(dataset, '-----------')\n",
        "    \n",
        "    tf_idf = True\n",
        "    adj, features, labels, n_classes =  datagen(dataset)\n",
        "    initial_feats = features.copy()\n",
        "    n, d = features.shape\n",
        "    norm_adj, features = preprocess_dataset(adj, features, \n",
        "                                            tf_idf=tf_idf,\n",
        "                                            sparse=False)\n",
        "    \n",
        "    ppmi_mat = ppmi(features)\n",
        "    \n",
        "    col_norm_adj, _ = preprocess_dataset(ppmi_mat, features, \n",
        "                                            tf_idf=tf_idf,\n",
        "                                            sparse=False)\n",
        "    \n",
        "    \n",
        "    n, d = features.shape\n",
        "    k = n_classes\n",
        "      \n",
        "      \n",
        "    metrics = {}\n",
        "    metrics['pmi'] = []\n",
        "    metrics['acc'] = []\n",
        "    metrics['nmi'] = []\n",
        "    metrics['ari'] = []\n",
        "    metrics['time'] = []\n",
        "    \n",
        "    def convert_sparse_matrix_to_sparse_tensor(X):\n",
        "      coo = X.tocoo()\n",
        "      indices = np.mat([coo.row, coo.col]).transpose()\n",
        "      return tf.SparseTensor(indices, coo.data, coo.shape)\n",
        "\n",
        "    norm_adj = convert_sparse_matrix_to_sparse_tensor(norm_adj.astype('float64'))\n",
        "    features = tf.convert_to_tensor(features.astype('float64'))\n",
        "    \n",
        "    x = features\n",
        "    for p in range(max_power):\n",
        "      for run in range(n_runs):\n",
        "        features = x\n",
        "        t0 = time()\n",
        "        features = features @ col_norm_adj\n",
        "        features = convolve(features, norm_adj, p)\n",
        "        \n",
        "        \n",
        "        \n",
        "        P, _ = run_model(features, c=2**-.5, k=k, coclustering=False)\n",
        "        \n",
        "        metrics['time'].append(time()-t0)\n",
        "        metrics['acc'].append(clustering_accuracy(labels, P)*100)\n",
        "        metrics['nmi'].append(nmi(labels, P)*100)\n",
        "        metrics['ari'].append(ari(labels, P)*100)\n",
        "\n",
        "      results = {\n",
        "          'mean': {k:(np.mean(v)).round(2) for k,v in metrics.items() }, \n",
        "          'std': {k:(np.std(v)).round(2) for k,v in metrics.items()}\n",
        "      }\n",
        "\n",
        "      means = results['mean']\n",
        "      std = results['std']\n",
        "\n",
        "\n",
        "      print(f\"{dataset} {p}\")\n",
        "      print(f\"{means['acc']}±{std['acc']} & {means['nmi']}±{std['nmi']} & {means['ari']}±{std['ari']}\", sep=',')\n",
        "      print(f\"time:\\n{means['time']}±{std['time']}\")"
      ],
      "metadata": {
        "id": "WONwnEo0YLWj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}